import pandas as pd
import numpy as np
from scipy.stats import multivariate_normal
from tqdm import tqdm
import math

import os

### ["x", "y", "velocity", "motion_angle", "cov1", "cov2", "cov3", "cov4", "weight", "motion_ratio", "decay_rate"]
# Standard format: velocity first, then motion_angle

############################### Read data ###################################
def read_test_data(datafile, dataset="MAGNI"):
    if dataset == "MAGNI":
        test_data = pd.read_csv(datafile)
        test_data = test_data.rename(columns={
            'Time': 'time',
            'ag_id': 'person_id',
            'speed': 'velocity',
            'theta_delta': 'motion_angle'
        })
        test_data['motion_angle'] = np.mod(test_data['motion_angle'], 2 * np.pi)
        test_data = test_data[['time', 'x', 'y', 'velocity', 'motion_angle']]
    elif dataset == "ATC":
        test_data = pd.read_csv(datafile, header=None)
        test_data.columns = ["time", "person_id", "x", "y", "velocity", "motion_angle"]
        test_data['motion_angle'] = np.mod(test_data['motion_angle'], 2 * np.pi)
        test_data = test_data[['time', 'x', 'y', 'velocity', 'motion_angle']]
    elif dataset == "MAPF":
        test_data = pd.read_csv(datafile, header=None)
        test_data.columns = ["time", "person_id", "x", "y", "velocity", "motion_angle"]
        test_data['motion_angle'] = np.mod(test_data['motion_angle'], 2 * np.pi)
        test_data = test_data[['time', 'x', 'y', 'velocity', 'motion_angle']]
    elif dataset == 'MAGNIv2':
        test_data = pd.read_csv(datafile, header=None)
        test_data.columns = ["time", "person_id", "x", "y", "velocity", "motion_angle"]
        test_data['motion_angle'] = np.mod(test_data['motion_angle'], 2 * np.pi)
        
    elif dataset == 'REALWORLD':
        test_data = pd.read_csv(datafile, header=None)
        test_data.columns = ["time", "person_id", "x", "y", "velocity", "motion_angle"]
        test_data['motion_angle'] = np.mod(test_data['motion_angle'], 2 * np.pi)
        
        # person_filter_id = [
        #     21031, 23130, 24012, 21244, 23110
        #     ]
        # # person_filter_id = [21244]
        # test_data = test_data[~test_data['person_id'].isin(person_filter_id)]
        
        test_data = test_data[['time', 'x', 'y', 'velocity', 'motion_angle']]
    return test_data


# which is generated by online MoD
def read_MoD_data_velocity(datafile):
    MoD = pd.read_csv(datafile, header=None)
    MoD.columns = ["x", "y", "velocity", "motion_angle", "cov1", "cov2", "cov3", "cov4", "weight", "motion_ratio"]
    MoD['motion_angle'] = np.mod(MoD['motion_angle'], 2 * np.pi)
    MoD['weight'] = MoD.groupby(['x', 'y'])['weight'].transform(lambda x: x / x.sum())
    
    return MoD


# which is generated by history, interval or window
def read_MoD_data_motionangle(datafile):    
    MoD = pd.read_csv(datafile, header=None)
    MoD.columns = ["x", "y", "motion_angle", "velocity",
                    "cov4", "cov2", "cov3", "cov1", "weight", "motion_ratio"]
    
    MoD = MoD[["x", "y", "velocity", "motion_angle", "cov1", "cov2", "cov3", "cov4", "weight", "motion_ratio"]]
    
    MoD["x"] = MoD["x"].astype(int)
    MoD["y"] = MoD["y"].astype(int)
    
    # MoD['x'] = MoD['x'].round()
    # MoD['y'] = MoD['y'].round()
    
    MoD['weight'] = MoD.groupby(['x', 'y'])['weight'].transform(lambda x: x / x.sum()) 
    
    return MoD


def round_to_nearest_half(x):
    return math.floor(x * 2) / 2

def read_MoD_data_motionangle_magni(datafile):    
    MoD = pd.read_csv(datafile, header=None)
    MoD.columns = ["x", "y", "motion_angle", "velocity",
                    "cov4", "cov2", "cov3", "cov1", "weight", "motion_ratio"]
    
    MoD = MoD[["x", "y", "velocity", "motion_angle", "cov1", "cov2", "cov3", "cov4", "weight", "motion_ratio"]]
    MoD['weight'] = MoD.groupby(['x', 'y'])['weight'].transform(lambda x: x / x.sum()) 

    MoD['x'] = MoD['x'].apply(round_to_nearest_half)
    MoD['y'] = MoD['y'].apply(round_to_nearest_half)

    # MoD['x'] = MoD['x'].map(lambda v: f"{v:.1f}")
    # MoD['y'] = MoD['y'].map(lambda v: f"{v:.1f}")
    MoD.to_csv("test1111.csv", index=False, header=False)

    return MoD


# which is generated by online MoD, for MAPF benchmark dataset
def read_MoD_data_decay_version(datafile):
    MoD = pd.read_csv(datafile, header=None)
    MoD.columns = ["x", "y", "velocity", "motion_angle", "cov1", "cov2", "cov3", "cov4", "weight", "motion_ratio", "decay_rate"]
    
    MoD['weight'] = MoD.groupby(['x', 'y'])['weight'].transform(lambda x: x / x.sum())
    MoD['motion_angle'] = np.mod(MoD['motion_angle'], 2 * np.pi)
    
    MoD["x"] = MoD["x"].astype(int)
    MoD["y"] = MoD["y"].astype(int)
    
    return MoD

def read_MoD_data_decay_version_atc(datafile):
    MoD = pd.read_csv(datafile, header=None)
    MoD.columns = ["x", "y", "velocity", "motion_angle", "cov1", "cov2", "cov3", "cov4", "weight", "motion_ratio", "decay_rate"]
    
    MoD['weight'] = MoD.groupby(['x', 'y'])['weight'].transform(lambda x: x / x.sum())
    MoD['motion_angle'] = np.mod(MoD['motion_angle'], 2 * np.pi)

    filter_condition = (MoD['cov1'] != 0) | (MoD['cov2'] != 0) | (MoD['cov3'] != 0) | (MoD['cov4'] != 0)
    MoD = MoD[filter_condition]

    return MoD

# which is generated by online MoD
def read_MoD_data_decay_version_magni(datafile):
    MoD = pd.read_csv(datafile, header=None)
    MoD.columns = ["x", "y", "velocity", "motion_angle", "cov1", "cov2", "cov3", "cov4", "weight", "motion_ratio", "decay_rate"]
    
    MoD['weight'] = MoD.groupby(['x', 'y'])['weight'].transform(lambda x: x / x.sum())
    MoD['motion_angle'] = np.mod(MoD['motion_angle'], 2 * np.pi)
    
    # MoD['x'] = MoD['x'].apply(round_to_nearest_half)
    # MoD['y'] = MoD['y'].apply(round_to_nearest_half)
    
    filter_condition = (MoD['cov1'] != 0) | (MoD['cov2'] != 0) | (MoD['cov3'] != 0) | (MoD['cov4'] != 0)
    MoD = MoD[filter_condition]
    
    return MoD

# which is generated by matlab
def read_cliff_map_data_obs_version(datafile):
    MoD = pd.read_csv(datafile, header=None)

    MoD.columns = ["x", "y", "motion_angle", "velocity",
                    "cov4", "cov2", "cov3", "cov1", "weight",
                    "observation_ratio", "motion_ratio"]
    
    MoD = MoD[["x", "y", "velocity", "motion_angle", "cov1", "cov2", "cov3", "cov4", "weight", "motion_ratio"]]
    MoD['motion_angle'] = np.mod(MoD['motion_angle'], 2 * np.pi)
    MoD['weight'] = MoD.groupby(['x', 'y'])['weight'].transform(lambda x: x / x.sum())

    return MoD
#############################################################################


############################### Check Cov ###################################
def is_valid_covariance(cov):
    # Check if the covariance matrix is valid (non-zero and positive definite)
    try:
        np.linalg.cholesky(cov)
        return True
    except np.linalg.LinAlgError:
        return False
    
    
def is_too_narrow(cov, threshold=1e-6):
    determinant = np.linalg.det(cov)
    return determinant < threshold


def regularize_covariance(cov, epsilon=1e-6):
    cov += epsilon * np.eye(cov.shape[0])
    return cov
#############################################################################


############################### GMM #########################################
def nll_of_point(gmm_components, point):
    prob = 0
    for _, component in gmm_components.iterrows():
        mean = [component['velocity'], component['motion_angle']]
        cov = [[component['cov1'], component['cov2']], [component['cov3'], component['cov4']]]
        weight = component['weight']
        # prob += weight * multivariate_normal.pdf([point['velocity'], point['motion_angle']], mean, cov)
        prob_res = []
        for wrap_num in [-1, 0, 1]:
            try:
                prob_res_wrap = weight * multivariate_normal.pdf([point['velocity'], point['motion_angle'] + 2 * np.pi * wrap_num], mean, cov, allow_singular=True)
            except:
                prob_res_wrap = 0
            prob_res.append(prob_res_wrap)
            prob += prob_res_wrap
        
    if prob < 1e-9:
        # return None
        prob = 1e-9
        nll = -np.log(prob)
    else:
        nll = -np.log(prob)

    return nll


def find_closest_location(locations, point, threshold):
    min_dist = float('inf')
    closest_loc = None
    for loc in locations:
        dist = np.sqrt((loc[0] - point['x'])**2 + (loc[1] - point['y'])**2)
        if dist < min_dist:
            min_dist = dist
            closest_loc = loc
            
    if min_dist > threshold:
        return None
            
    return closest_loc


def preprocess_gmm_data(gmm_data):
    locations = gmm_data[['x', 'y']].drop_duplicates().values
    gmm_dict = {}
    for loc in locations:
        loc_data = gmm_data[(gmm_data['x'] == loc[0]) & (gmm_data['y'] == loc[1])]
        valid_rows = []
        for _, row in loc_data.iterrows():
            cov = np.array([[row['cov1'], row['cov2']], [row['cov3'], row['cov4']]])
            # if row['cov1'] > 0.03 and row['weight'] > 0 and is_valid_covariance(cov) and (not is_too_narrow(cov)):
            if row['weight'] > 0 and is_valid_covariance(cov) and (not is_too_narrow(cov)):
            # if row['weight'] > 0 and is_valid_covariance(cov):
                # if is_too_narrow(cov):
                #     cov = regularize_covariance(cov, epsilon=1e-6)
                row['cov1'], row['cov2'], row['cov3'], row['cov4'] = cov.flatten()
                valid_rows.append(row)
        
        if valid_rows:
            loc_data = pd.DataFrame(valid_rows)
            weights = loc_data['weight'].values
            weights /= weights.sum()  # Normalize the weights
            loc_data['weight'] = weights
            gmm_dict[(loc[0], loc[1])] = loc_data
        
    return gmm_dict

### Return nlls: list of nll of test_data, 
# not_find: number of points in test_data that do not find the closest MoD under threshold, 
# average_nll: average nll of test_data
def compute_nll(test_data, MoD_data, threshold):
    gmm_dict = preprocess_gmm_data(MoD_data)

    locations = list(gmm_dict.keys())
    nlls = []

    not_find = 0
    
    for _, point in tqdm(test_data.iterrows(), total=test_data.shape[0]):
        closest_loc = find_closest_location(locations, point, threshold)
        if closest_loc is None:
            # print(f"Cannot find the closest location for point {point}")
            not_find += 1
            nlls.append(-np.log(1e-9))
            continue
        else:
            gmm_components = gmm_dict[closest_loc]
            nll = nll_of_point(gmm_components, point)
            if nll is not None:
                nlls.append(nll)
        
    average_nll = np.mean(nlls)

    return nlls, not_find, average_nll
